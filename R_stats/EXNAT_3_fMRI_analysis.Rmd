---
title: "EXNAT_3_fMRI_analysis"
author: "Sandra Martin"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r file setup, echo = FALSE}
rm(list = ls()) # clear environment
knitr::opts_chunk$set() # set default options for all code blocks in this document
options(scipen = 999) # don't use scientific notation for very large or small numbers
```

# Load packages
```{r packages, echo = FALSE, message = FALSE, warning = TRUE}
# Create a list with needed libraries
pkgs <- c("here", # for working with relative paths
          "ggeffects", # create data frames of marginal effects for 'ggplot' from model outputs
          "stringr", # for getting substrings
          "ggplot2", # for plots
          "dplyr", # for replacing multiple values in vector with different values
          "reshape2", # for reshaping df format
          "tidyverse", # for aggregating
          "psycho", # for computing d-prime values
          "devtools", # for getting packages from github
          "gtools", # for getting tuples from list
          "see", # for plotting residuals of lme4 model
          "performance", # needed for plotting the lme4 model output summary
          "lme4", # also for linear mixed models, but no p-values in the summary
          "lmerTest", # extension of lme4 - for tab_model() for showing lme4 model results
          "modelsummary", # for printing model fit measures
          "car", # for basic Anova() function & aGSIFs (= basically weird adjusted VIFs)
          "sjPlot", # for running Anova & showing results in a HTML table in the Viewer
          "colorspace", # for beautiful and balanced color palettes
          "readxl", # reading in Excel tables
          "interactions") # for simple slopes analysis


# Load each listed library, check if it's already installed
# and install if necessary
for (pkg in pkgs){
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

# install package with lists of stop words from github
devtools::install_github("quanteda/stopwords")
library(stopwords)
```

# Load d-prime function from different R script:
```{r source d-prime function, echo = FALSE}
source(here::here("get_dprime.R")) # get script
```


# Settings for plots
```{r Theme for plots, echo = FALSE}
apatheme <- theme_bw()+
  theme(plot.title = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        text = element_text(family = 'sans',size = 14)) #panel.grid.major=element_blank(),

today <- Sys.Date()
today <- format(today, format="%y%m%d")


# Custom colour palettes for the plots:

# for distinguishing between paced and unpaced task:
palet_task_condition <- c("#D2BFE7",
                      "#977FB2") 
palet_task_condition_lines <- c("#ad8bd3",
                      "#7B5EC6") 

# for distinguishing between cognitive load conditions: 
# palet_load <- c("#575A7B", # Reading Only
#                 "#ED9201", # 1-back
#                 "#982126") # 2-back
palet_load <- c("#4B201D", "#9F4D48", "#F8A29E")

# for distinguishing between n-back tasks (single- & dual 1- and 2-back)
palet_dprimes = c("#ED9201", # 1-back Dual
                  "#fec160", # 1-back Single
                  "#982126", # 2-back Dual
                  "#d95459") # 2-back Single
palet_dprimes_lines = c("#cb7d00", # 1-back Dual
                        "#ED9201", # 1-back Single
                        "#982126", # 2-back Dual
                        "#d95459") # 2-back Single

# for simple slopes plots:
palet_simple_slopes = c("#585858", # n.s.
                        "#0c6ae8") # sign.

# For effects we don't really have colours for. Like age or surprisal.
palet_effects <- c("#365464") 

```


# Set path to datasets
```{r path setup, echo = FALSE}
# online and lab data should be all in 1 folder:
path_data_folder <- here::here("Raw_data/fMRI_experiment/")
```

# Read in data & do preprocessing on participant-level
In this chunk, we read in every dataset individually, compute reading times, reading speed, d-primes, comprehension question performance, and so on, 
add a few information on the words we used (e.g. surprisal scores on our 4 time scales, word frequencies, word lengths, punctuation,...), 
and then we exclude data on block level:
We only exclude data on block-level if performance measures indicate they didn't do the task(s) correctly.
```{r Read in data, message=FALSE, echo=FALSE}
# Get list of all .csv files in the data folder
file_list <- list.files(path = path_data_folder, pattern='.csv')

# Placeholders df for demographics, questions & text data
df_all_data        <- data.frame()
df_comprehension_Qs <- data.frame()

# Loop files in my file list aka directory
for (i in 1:length(file_list)) {

  # PREPARE FILE FOR PREPROCESSING
  # Read in current file
  subj_df <- read.csv(paste(path_data_folder, file_list[i],sep = ""), sep = ",")

  # If the df is separated by a semicolon instead of a comma, we only get
  # one messed-up column. In this case, use semicolon as separator for csv:
  if (ncol(subj_df) == 1) {
   subj_df <- read.csv(paste(path_data_folder, file_list[i],sep = ""), sep = ";")
  }
  
  id <- subj_df$participant[1]
  
  # print message
  message(paste(i, " - Reading in file ", file_list[i], ", participant ID: ", id, sep = ""))

  # add column with info on whether participant should be excluded
  excl <- FALSE
  
  # rename block_kind column
  subj_df <- subj_df %>% 
    rename(nback_level = n.back_level)

  # GET RAW TEXT & N-BACK DATA

  # get words & information on the text from data
  subj_df <- subj_df[, c("colour", "target", "nback_response", "nback_RT", "duration", "text_nr", "trial_nr", "block_nr_exp", "run_nr", "block_nr_run", "block_name", "nback_level", "word", "question", "button_pressed", "chosen_ans", "ans_correct", "RT_per_rectangle_oneback_single", "RT_per_rectangle_twoback_single", "RT_per_letter_baseline", "RT_per_letter_oneback_dual", "RT_per_letter_twoback_dual", "participant", "session", "date")]
  
  # remove weird empty rows
  subj_df <- subset(subj_df, !is.na(run_nr))

  
  ### ADD NUMBERED BLOCK NAMES ####

  # # We have each main block twice, but I would like to exclude outliers by block and not by condition.
  # # Reason: Reading times in second block might be slightly different than in first block, so don't mix them up.
  # 
  change_df <- subj_df %>%
    filter(!is.na(trial_nr)) %>%
    group_by(run_nr, block_nr_run, block_name) %>%
    count() %>%
    ungroup() %>%
    group_by(block_name) %>%
    mutate(Index=1:n()) %>%
    mutate(block_names_numbered = paste(block_name, Index, sep = "_")) %>%
    select(-c(Index, n))

  subj_df <- full_join(subj_df, change_df)

  ### MERGE INFO ON RT PER LETTER INTO ONE COLUMN ####
  subset_df <- subj_df %>% 
    select(c(block_name, starts_with("RT_per_"))) %>% 
    mutate(Paced_RT = case_when(
      block_name == "Reading_Baseline_main_no_click" ~ RT_per_letter_baseline[!is.na(subj_df$RT_per_letter_baseline)][1],
      block_name == "Reading_pseudotext_no_click" ~ RT_per_letter_baseline[!is.na(subj_df$RT_per_letter_baseline)][1],
      block_name == "1back_dual_main_no_click" ~ RT_per_letter_oneback_dual[!is.na(subj_df$RT_per_letter_oneback_dual)][1],
      block_name == "2back_dual_main_no_click" ~ RT_per_letter_twoback_dual[!is.na(subj_df$RT_per_letter_twoback_dual)][1],
      block_name == "1back_single_main_no_click" ~ RT_per_rectangle_oneback_single[!is.na(subj_df$RT_per_rectangle_oneback_single)][1],
      block_name == "2back_single_main_no_click" ~ RT_per_letter_twoback_dual[!is.na(subj_df$RT_per_letter_twoback_dual)][1],
    )) %>% 
    select(c(block_name, Paced_RT)) %>% 
    distinct()
  
  subj_df <- full_join(subj_df, subset_df)

  
  # GET ADDITIONAL INFORMATION ON THE TEXTS:

  ### PUNCTUATION ####

  # Edit the texts a bit. Currently, we have words & punctuation
  # mixed up. Would be nice if we had one word column
  # and some others with info on punctuation.

  punctuation <- c(rep("", times = length(subj_df$participant)))

  # get all .
  punctuation[grep("[.]", subj_df$word)] <- "point"
  # get all ?
  punctuation[grep("[?]", subj_df$word)] <- "question_mark"
  # get all !
  punctuation[grep("[!]", subj_df$word)] <- "exclamation_mark"
  # get all ,
  punctuation[grep("[,]", subj_df$word)] <- "comma"
  # get all ;
  punctuation[grep("[;]", subj_df$word)] <- "semicolon"
  # get all :
  punctuation[grep("[:]", subj_df$word)] <- "colon"

  # get all "
  # This is tricky for various reasons:

  # 1. there could be quotes directly after or
  # before a point for example

  # Idea: Separate column for quotes
  quotes <- c(rep("", times = length(subj_df$participant)))

  # 2. I want to differentiate between opening and closing quotes,
  # but they look the same.

  # Idea: make them all "opening quotes" and change every second one to
  # "closing_quote". This should do the trick.
  quotes[grep("\"", subj_df$word)] <- "opening_quote"
  opening <- TRUE

  # loop list of quotes
  for (x in 1:length(quotes)){
    # get current value in vector "quotes" & current word
    curr_val  <- quotes[x]
    curr_word <- subj_df$word[x]

    if (curr_val == "opening_quote" & # if the current value is not empty
        opening == TRUE & # and we set "opening" to TRUE
        grepl("\"[A-Za-z0-9]+\"", curr_word) == FALSE){ # and there are not 2 quotes around the word

      # leave it as is, next quote is the closing one
      opening <- FALSE

      # if we have a quote, but it has to be a closing one, change it accordingly
    } else if (curr_val == "opening_quote" &
               opening == FALSE &
               grepl("\"[A-Za-z0-9]+\"", curr_word) == FALSE){
      # change value to closing quote
      quotes[x] <- "closing_quote"
      # next quote is the opening one
      opening <- TRUE
      # if it's a quote around a single word, mark as "both"
    } else if (curr_val == "opening_quote" &
               grepl("\"[A-Za-z0-9]+\"", curr_word) == TRUE){
      # change value to both
      quotes[x] <- "both"
    }
  }

  # remove all punctuation
  word_single <- gsub('[[:punct:] ]+',' ', subj_df$word)
  # "trim" away spaces before and after words
  word_single <- trimws(word_single, which = c("left"))
  word_single <- trimws(word_single, which = c("right"))
  # put dashes into spaces between words (words like "ice-cream" for example)
  word_single <- gsub(" ", "-", word_single, fixed = TRUE)

  # get word length for single words
  word_length_single <- nchar(word_single)


  ### N-BACK RESPONSES ####

  # check if we had an n-back reaction while the word was shown
  # create vector with only FALSEs
  reaction <- c(rep(FALSE, times = length(subj_df$participant)))
  # check where in subj_df we have responses (hit or false alarm) and change value in reaction vector to TRUE at that index
  reaction[which(subj_df$nback_response == "hit" | subj_df$nback_response == "false alarm")] <- TRUE

  # APPEND NEW COLUMNS
  # append punctuation and quotes column, column with words without punctuation
  # column with length of each the single words and reaction column as
  # additional columns to subj_df:
  subj_df <- data.frame(cbind(subj_df, word_single, punctuation,
                              quotes, word_length_single, reaction))

  ### STOP WORDS ####

  # Now also get previous reaction and stop words.
  # First, append empty columns to df:
  subj_df[, c("previous_reaction", "previous_duration", "stop_word")] <- ""

  # get list of German stop_words
  stop_words <- stopwords("de", source = "snowball")
  # snowball = stopwords list based on the Snowball stemmer's word lists.
  # --> https://snowballstem.org/texts/introduction.html

  # now loop rows and gradually fill in the empty columns.
  for (idx in 1:length(subj_df$participant)){
    # if it's the first trial of a new block, go to next iteration
    # because in this case we don't have previous data.
    # Also skip the following part if it's a block without text.
    if (subj_df$trial_nr[idx] > 1 && !is.na(subj_df$trial_nr[idx])){
      # fill in the missing data by getting the values from the previous trial:
      subj_df$previous_duration[idx]    <- subj_df$duration[idx-1]
      subj_df$previous_reaction[idx]    <- subj_df$reaction[idx-1]
    }

    if (subj_df$word_single[idx] %in% stop_words){
      # check if current word is a stop word or not
      subj_df$stop_word[idx] <- TRUE 
      }
    else {
      subj_df$stop_word[idx] <- FALSE
    }
  }
  
  ### WORD FREQUENCIES & SURPRISAL SCORES ####

  # Include word frequencies & surprisal scores for each word

  # append "empty" word frequency & surprisal score columns to df
  # to do so, create a vector of column names
  col_names <- c("word_frequency", paste0("surprisal_", c(2)))
  # then add "empty" columns to data frame subj_df
  subj_df[, col_names] <- 0

  # get word frequency & surprisal scores for each word
  # load word freq df
  word_freqs_df = read.csv(here::here("word_frequencies/Word_freqs.csv"), sep = ";", header = TRUE)[2:4]
  # Explanation: We compute the word frequencies using another
  #              script which is called "calculate_word_frequencies.py".
  #              You can find it in the Analysis folder.
  #              --> word frequencies were taken from this python package:
  #              Speer, R., Chin, J., Lin, A., Jewett, S., & Nathan, L. (2018, October 3).
  #              LuminosoInsight/wordfreq: v2.2. Zenodo. https://doi.org/10.5281/zenodo.1443582


  # Load surprisal scores
  # load surprisal scores / similarity scores df with TS = context chunk size
#  surprisal_df = read.csv(here::here("surprisal scores/surprisal_scores_masked_context.csv"), sep = ",", header = TRUE)

  # Explanation: I computed the scores in Python using a German GPT-2 model. For each text you can select a context chunk of x words (e.g. 5 words)
  #              and predict the next word. For each possible continuation of your context, you get probabilities.
  #              If you get the probability for the actual word and compute the negative log of it, you have the surprisal score
  #              for your word on time scale x (e.g. 5 words = TS 5). The time scales are all highly correlated, which might be due to the fact that each
  #              time scale also includes context information from all lower timescales, so it's like a Russian doll situation.
  #              To deal with this problem, we masked all words that were already processed in lower time scales,
  #              so each time scale only uses the "new" parts of the input chunk.


  # loop individual texts
  for (curr_text_nr in unique(subj_df$text_nr)){

    # in some blocks we don't have texts, so skip those
    # CAVE: currently also skipping text 10 because I still need to calculate values for this text
    if (curr_text_nr == "" | curr_text_nr == "text_10" | startsWith(curr_text_nr, "pseudo")){
      next
      # if it's a text block, though, assign word frequencies from csv
    } else {
      #print(curr_text_nr) # uncomment this if you'd like to show the texts each participant read

      # get word frequencies for current text nr
      curr_word_freqs <- subset(word_freqs_df, text_nr == curr_text_nr)$word_frequency
      # find out where in the subj_df text the text is located and add the word frequencies there
      subj_df[which(subj_df$text_nr == curr_text_nr & subj_df$question == ""),]$word_frequency <- curr_word_freqs

      # Do the same for the surprisal scores.
#      curr_surprisals <- subset(surprisal_df, text_nr == curr_text_nr)

      # find out where in the subj_df text the current text nr is located
#      curr_row <- which(subj_df$text_nr == curr_text_nr & subj_df$question == "")

      # add the surprisal scores (untransformed & orthogonalized scores) there
#      subj_df[curr_row, c("surprisal_1", "surprisal_4",
#                          "surprisal_12", "surprisal_60")]  <- curr_surprisals[c("surprisal_1", "surprisal_4",
#                                                                                 "surprisal_12",  "surprisal_60")]
      #subj_df[curr_row, c("surprisal_1_ortho", "surprisal_2_ortho",
      #                    "surprisal_8_ortho", "surprisal_16_ortho",
      #                    "surprisal_32_ortho")]                     <- curr_surprisals[c("surprisal_1_ortho", "surprisal_2_ortho",
      #                                                                                    "surprisal_8_ortho", "surprisal_16_ortho",
      #                                                                                    "surprisal_32_ortho")]

    }# END if
  }# END loop texts

  
  ### GET SURPRISAL SCORES OF PREVIOUS WORD ####
  # CURRENTLY NOT INCLUDED
  # --> basically do the same again as before, but add surprisal scores for current for to row of next word.

#   # append "empty" word frequency & surprisal score columns to df
#   # to do so, create a vector of column names
#   col_names <- c("word_frequency_previous_word", paste0("previous_surprisal_", c(1, 4, 12, 60)))
#   # then add "empty" columns to data frame subj_df
#   subj_df[, col_names] <- 0
# 
#   # loop individual texts
#   for (curr_text_nr in unique(subj_df$text_nr)){
# 
#     # in some blocks we don't have texts, so skip those
#     if (curr_text_nr == "" | curr_text_nr == "text_10" | startsWith(curr_text_nr, "pseudo")){
#       next
#       # if it's a text block, though, assign word frequencies from csv
#     } else {
#       #print(curr_text_nr) # uncomment this if you'd like to show the texts each participant read
# 
#       # get word frequencies for current text nr
#       curr_word_freqs <- subset(word_freqs_df, text_nr == curr_text_nr)$word_frequency
# 
#       # now this is where we do it differently than before: Remove the last value and add 1 NA
#       # at idx = 1 of the vector, so the values are moved by 1 position.
#       curr_word_freqs <- c(NA, curr_word_freqs[-length(curr_word_freqs)])
# 
#       # find out where in the subj_df text the text is located and add the word frequencies there
#       subj_df[which(subj_df$text_nr == curr_text_nr & subj_df$question == ""),]$word_frequency_previous_word <- curr_word_freqs
# 
#       # Do the same for the surprisal scores:
# #      curr_surprisals <- subset(surprisal_df, text_nr == curr_text_nr)[ , c("surprisal_1", "surprisal_4",
# #                                                                            "surprisal_12", "surprisal_60")]
# 
#       # add 1 NA to the start of the vectors, remove last value in each
# #      curr_surprisals <- rbind(c(NA, NA, NA, NA), curr_surprisals[-nrow(curr_surprisals), ])
# 
#       # find out where in the subj_df text the current text nr is located
# #      curr_row <- which(subj_df$text_nr == curr_text_nr & subj_df$question == "")
# 
#       # add the surprisal scores (untransformed & orthogonalized scores) there
# #      subj_df[curr_row, c("previous_surprisal_1", "previous_surprisal_4",
# #                          "previous_surprisal_12","previous_surprisal_60")] <- curr_surprisals
# 
#     }# END if
#   }# END loop texts

  ### CHANGE ORDER OF DATAFRAME ####
  # Move around the columns a bit:
  col_order <- c("participant", "nback_level", "block_nr_exp", "run_nr", "block_nr_run", "block_name",
                 "trial_nr", "text_nr", "word", "duration",
                 "colour", "target", "button_pressed", "nback_response", "nback_RT", "reaction",
                 "word_single", "word_length_single", "word_frequency", "Paced_RT",
#                 "surprisal_2", "previous_surprisal_2", "word_frequency_previous_word", 
                 "previous_duration", "previous_reaction",
                 "session", "question", "chosen_ans", "ans_correct", "stop_word", "punctuation", "quotes", "date")

  subj_df <- subj_df[, col_order]
  
  ### GET PERFORMANCE MEASURES ####

  #### COMPREHENSION QUESTION PERFORMANCE ####
  
  # get question data:
  Q_df <- subset(subj_df, question != "")[,c("question", "chosen_ans", "ans_correct", "text_nr", "block_name", "participant")]

  Q_df$ans_correct <- as.logical(Q_df$ans_correct)
  
  # calculate accuracy per text
  Q_df <- Q_df %>% 
   group_by(text_nr) %>% 
    mutate(question_acc = sum(ans_correct[ans_correct == TRUE])/3)
  
  Q_subset <- Q_df %>% 
    select(c(text_nr, question_acc)) %>% 
    distinct

  # append Q_subset to bigger df for all participants
  df_comprehension_Qs <- as.data.frame(rbind(df_comprehension_Qs, Q_df))
  
  # append overall accuracy per text to subject df
  subj_df <- full_join(subj_df, Q_subset) %>% 
    select(-c(question, chosen_ans, ans_correct)) %>% 
    filter(!is.na(duration))


  ### N-BACK PERFORMANCE ####

  # set all durations in n-back trials without response to NA
  subj_df[which(subj_df$reaction == F), ]$nback_RT <- NA

  # create vector of n-back blocks in experiment
  nback_block_names <- subj_df %>% 
    filter(str_detect(nback_level, "^1") | str_detect(nback_level, "^2")) %>% 
    distinct(block_nr_exp) %>% 
    pull(block_nr_exp)

  # append empty d-prime column to df:
  subj_df$dprime <- c(rep(NA, times = length(subj_df$word)))

  # loop block names:
  for (nback_block in nback_block_names){

    # get data for current block
    curr_block <- subset(subj_df, block_nr_exp == nback_block)

    # remove trials where the RT was way too fast
    # (= participant reacted by accident)
    curr_block <- subset(curr_block, (nback_RT >= 100 | is.na(nback_RT)))

    # if there are still some trials left (let's say at least 5), compute d-prime
    if (length(curr_block$participant) > 5){
        # compute d-prime & add to bigger df
        d_prime <- get_dprime(curr_block$nback_response)
        subj_df$dprime[subj_df$block_nr_exp == nback_block] <- d_prime
    } # END if loop - check if there are still enough trials left
  }# END loop - compute d-primes
  
  # We'll use the mean d-prime from 1-back and 2-back single task main blocks
  # as a working memory measure for each participant. So get mean & append it as a new column to the df.
  subj_df$mean_dprime_singletasks <- mean(c(unique(subj_df[which(subj_df$block_name =="1back_single_main_no_click"),]$dprime),
                                            unique(subj_df[which(subj_df$nback_level =="2back_single_main_no_click"),]$dprime)))
  
  ### MARK PARTICIPANTS/BLOCKS FOR EXCLUSION:

  # append "empty" exclude trial / exclude participant columns to df
  subj_df$excl_trial       <- FALSE

  ### EXCLUDE SINGLE BLOCKS BASED ON COMPREHENSION QUESTION PERFORMANCE: ####
  
  # Idea: only exclude on block level, don't exclude full datasets because they messed up in 1 condition or so.
  # To identify these blocks, we look for a question accuracy of 0

  subj_df$excl_trial[subj_df$question_acc == 0] <- TRUE
  message(paste("excluded", length(subj_df[which(subj_df$question_acc == 0), "excl_trial"]), "trials because accuracy was 0%", sep = " "))
  
  ### EXCLUDE SINGLE BLOCKS BASED ON N-BACK TASK PERFORMANCE: ####

  # Check blocks with n-back tasks: 
  # Exclude block if the participant didn't do the n-back task (i.e. if they always/never pressed the target button).
  # We're excluding based on the d-prime values.
  # In the dual-task (main) blocks, we always had 50 targets & 250 non-targets. 
  # The d-prime for 0 hits and 0 false alarms is 0. If they always pressed the button and have 50 hits and 250 false alarms, the d-prime is also 0.
  # This means that they should have a d-prime above 0 if they at least attempted to do the task or below 0 if they attempted to do the 
  # task but did it the wrong way around (i.e.reacted if trial was a non-target trial and didn't react if trial was a target trial).
  
  # --> exclude all blocks with d-primes == 0.
  
  message(paste("excluded", length(subj_df[which(subj_df$dprime == 0), "excl_trial"]), "trials because d-prime was 0", sep = " "))
  subj_df[which(subj_df$dprime == 0), "excl_trial"] <- TRUE

  # append subj_df chunk to df_text_data where we collect the data of all participants we want to keep
  df_all_data <- as.data.frame(rbind(df_all_data, subj_df))

  message("------------------------")

}# END READ IN DATASETS

# exclude all trials that we marked for exclusion:
#df_all_data <- subset(df_all_data, excl_trial == FALSE)


# clean up!
#rm(list=ls()[! ls() %in% c("df_all_data", "df_comprehension_Qs", 
#                           "apatheme", "palet_load", 
#                           "palet_dprimes", "palet_dprimes_lines", 
#                           "palet_lab_online", "palet9", "today")])
```

# Add demographic data to dataframe
```{r}
# read in table with demographic data
df_demographics <- readxl::read_excel(here::here("Raw_data/EXNAT_3_Participants.xlsx"))
df_demographics <- df_demographics[1:91,c("Subject", 
                                          "Age", 
                                          "Sex", 
                                          "RT_per_letter_baseline", 
                                          "RT_per_rect_1back_single", 
                                          "RT_per_rect_2back_single",
                                          "RT_per_letter_0back_dual",
                                          "RT_per_letter_1bck_dual",
                                          "RT_per_letter_2bck_dual",
                                          "MOCA",
                                          "DSST",
                                          "TMT-A",
                                          "TMT-B",
                                          "Reading_span")]
df_demographics <- df_demographics %>% 
  rename(participant = Subject,
         age = Age,
         sex = Sex)

df_all_data <- full_join(df_all_data, df_demographics)
exp_all_data <- df_all_data
```


# Save raw data in folder "RData"
```{r save raw data, echo = FALSE}
# save df
save(exp_all_data, file = here::here("RData/exp_all_data.RData"))

# save comp question df
save(df_comprehension_Qs, file = here::here("RData/exp_df_comprehension_questions.RData"))
```

# Find & exclude outliers using z-sqrt-POMS-transformation
# do not run until final transformation method is decided upon
```{r find and exclude outlier trials, echo = FALSE}
# DOESN'T SEEM TO BE A VERY VALID APPROACH WITH PACED DURATIONS SINCE THEY ARE ALREADY VERY MUCH FOCUSED ON THE INDIVIDUAL'S PERFORMANCE. I BELIEVE WE MIGHT NOT HAVE TO EXCLUDE ANY OUTLIERS AT ALL IN THE PACED BLOCKS SINCE TECHNICALLY OUTLIERS ARE NOT A THING ANYMORE.

# only use data from the main blocks from now on:
df_text_data_clean <- subset(df_all_data, block_name == "Reading_Baseline_main_no_click" | block_name == "Reading_pseudotext_no_click" | block_name == "1back_dual_main_no_click" | block_name == "2back_dual_main_no_click")

message("Nr of trials before exclusion of trials: ", length(df_text_data_clean$participant))

### EXCLUDE BREAKS ####
# --> Exclude all trials where participants took more than 5 seconds (arbitrary value)
# It's also a sanity check since this shouldn't be possible in EXNAT-3 due to introduced time-out values on trial basis
message(paste(length(subset(df_text_data_clean, duration > 5000)$ID), " trials containing breaks were excluded from further analysis", sep = ""))

df_text_data_clean <- subset(df_text_data_clean, duration <= 5000)


# Plot distribution of reading times 
densityplot(df_text_data_clean$duration)


### TRANSFORM RT DATA ####

# transf_val = sqrt (  ( x - sample_min ) / ( sample_max - sample_min)  )
# --> after this, z-transform all values and exclude all values exceeding a value of ± 2 (or ± 3, but they used 2 in the paper)

# This is basically a POMS (Little (2013), read in Moeller (2013)) transformation where you get
# the square root of the output afterwards and z-transform everything.

# Careful, normally, if you wanted to use the transformed values for group comparisons later, 
# you'd have to to the transformation across participants & conditions, so everything gets "pulled" into the same range.
# If you do this for each subject & condition separately, you can't compare means
# anymore, because every subset of data would have its own scale.

# Here, we only want to use the transformation for identifying & excluding outliers, so we don't really care about this problem. 
# So we can do this on block level, so we don't exclude more trials from "slower" 2-back blocks than from the BL for example.

# create new column in df_text_data_clean for the transformed RTs:
df_text_data_clean$tmp_transformed_RTs <- NA 

# get min reading time
# sample_min <- min(df_text_data$duration)
sample_min <- 0 # use smallest possible value here (this is described in a book by Little, 2013) - in this case: 0 words / 100 ms

# loop participants:
message("start excluding trials (block-wise)")
for (curr_id in unique(df_text_data_clean$participant)){
  
  message("current participant: ", curr_id)
  
  # create placeholders to count excluded trials / condition
  trials_excluded_BL    <- 0
  trials_excluded_1back <- 0
  trials_excluded_2back <- 0
  
  # get data of current participant:
  curr_df <- subset(df_text_data_clean, participant == curr_id)

  # loop blocks:
  for (curr_block in unique(curr_df$block_nr_exp)){
  
    # get data of current block:
    curr_block_data <- subset(curr_df, block_nr_exp == curr_block)

    # get max reading time (use sample maximum here)
    sample_max <- max(curr_block_data$duration)

    # do sqrt(POMS) transform of raw reading time values
    duration_standardized <- sqrt((curr_block_data$duration - sample_min) / (sample_max - sample_min))

    # z-transform reading data
    duration_standardized <- as.vector(scale(duration_standardized, center = T, scale = T))

    # put the transformed values into the big df with data of all participants:
    df_text_data_clean[which(df_text_data_clean$participant == curr_id & 
                             df_text_data_clean$block_nr_exp == curr_block), ]$tmp_transformed_RTs <- duration_standardized
    
    # check how many trials we have that are < -2 or > 2:
    trials_excluded <- length(which(duration_standardized > 3 | duration_standardized < -3))
  }# END LOOP BlOCKS
} # END LOOP PARTICIPANTS


# get index of all values < - 2 or > 2 & actually exclude those trials from the dataframe
excl_row_idx <- which(df_text_data_clean$tmp_transformed_RTs < -3 | df_text_data_clean$tmp_transformed_RTs > 3)

# check how the corresponding untransformed RTs look like so we get a feeling for what's being excluded:
# densityplot(df_text_data_clean[excl_row_idx, "duration"])

# kick out the outliers:
df_text_data_clean[excl_row_idx, "excl_trial"] <- TRUE
# total number of excluded trials
excl_trials <- sum(df_text_data_clean$excl_trial[df_text_data_clean$excl_trial == TRUE])
df_text_data_clean <- subset(df_text_data_clean, excl_trial == FALSE)

# drop column with the z-sqrt-POMS-transformed data, we don't need it anymore.  
df_text_data_clean$tmp_transformed_RTs <- NULL
```

# log-transform reading times
```{r log-transform reading times, echo = FALSE}
# Idea: Ignore that we just transformed our reading times for outlier exclusion. 
# We now take the raw reading times again and log-transform them.

# use natural logarithm (base e) here:
df_all_data$reading_times_log <- log(df_all_data$duration)
# to reverse the log transformation and get the data back to the original scale (for easier interpretation of the results), 
# use this: exp(log_transformed_data)

# plot the distribution: 
library(lattice)
densityplot(df_all_data$reading_times_log)

# make sure we update the values in the column previous_duration as well:
df_all_data$previous_reading_times_log <- log(as.numeric(df_all_data$previous_duration))

```

# Some renaming of columns
```{r}
# name df differently:
df_clean <- df_all_data

# rename "block_kind" column 
names(df_clean)[which(names(df_clean) == "nback_level")] <- "cognitive_load"

# recode values in cognitive_load column
df_clean <- df_clean %>% 
  mutate(cognitive_load = case_when(
    str_detect(cognitive_load, "None") ~ "Reading only",
    str_detect(cognitive_load, "1") ~ "1-back",
    str_detect(cognitive_load, "2") ~ "2-back",
  ))

# turn the values into factors and set their order
df_clean$cognitive_load <- factor(df_clean$cognitive_load,
                                  levels = c("Reading only", "1-back", "2-back"))

# save df
save(df_clean, file = here::here("RData/df_clean_all_subjects.RData"))
```

# Plots
## Plot RTs by Task and Condition
```{r plot rt by task and cond}
plot_df <- subset(df_clean, reaction == F)

#---- RTs by Task and Condition ----
plot_RT_task <- ggplot(plot_df, aes(x = cognitive_load, y = duration)) +
                      #geom_violinhalf(flip = c(1,3,5)) +
                      geom_boxplot() +
                      #geom_point2(plot_surprisal_df_summary, mapping = aes(x = Cognitive_Load, y = Reaction_time)) +
                      scale_color_manual(values = palet_task_condition_lines) +
                      scale_fill_manual(values = palet_task_condition) +
                      #facet_wrap(~ Surprisal_TS, ncol = 4) +
                      theme_classic(14)
                      #theme(legend.position = "none",
                      #      strip.background.x=element_rect(color = NA))
plot_RT_task
# save plot
cowplot::save_plot(plot_RT_task, file = here::here(paste0("Plots/RT_task_cond", today, ".pdf")))
```

## Plot RTs by Surprisal TS, Task & Condition (only dual-task blocks)
```{r plot RTs by Surprisal TS & Condition, message = FALSE, warning = FALSE}
plot_surprisal_df <- subset(df_clean, reaction == F) # exclude trials where you had a reaction
plot_surprisal_df <- pivot_longer(plot_surprisal_df, 
                                  cols = c("surprisal_1", "surprisal_4",
                                           "surprisal_12", "surprisal_60"),  
                                  names_to = "surprisal_ts", 
                                  values_to = "surprisal") %>% 
  select(c(surprisal_ts, participant, surprisal, cognitive_load, duration)) 

plot_surprisal_df$surprisal <- round(plot_surprisal_df$surprisal, digits = 0)
# Now group by participant, cognitive_load & time scale
plot_surprisal_df <- plot_surprisal_df %>% 
                    #group_by(participant, cognitive_load, surprisal_ts, surprisal) %>% 
                    #summarise(mean_rt = mean(duration)) %>% 
                            rename(Surprisal_TS = surprisal_ts, 
                                   Subject = participant,
                                   Surprisal = surprisal,
                                   Cognitive_Load = cognitive_load,
                                   Reaction_time = duration)

plot_surprisal_df$Surprisal_TS <- factor(plot_surprisal_df$Surprisal_TS, levels = c("surprisal_1", "surprisal_4", "surprisal_12", "surprisal_60"))

#plot_surprisal_df_summary <- plot_surprisal_df %>% 
#  group_by(Subject, Cognitive_Load) %>% 
#  summarise(Reaction_time = mean(Reaction_time))

#---- Plot RTs by Surprisal x Task ----
plot_RT_surprisal_task <- ggplot(plot_surprisal_df, aes(x = Surprisal, y = Reaction_time)) +
                      geom_smooth(method = lm, na.rm = T) +
                      #geom_point2(plot_surprisal_df_summary, mapping = aes(x = Cognitive_Load, y = Reaction_time)) +
                      #scale_color_manual(values = palet_task_condition) +
                      #scale_fill_manual(values = palet_task_condition) +
                      scale_x_continuous(breaks = scales::pretty_breaks(n = 5)) +
                      facet_wrap(~ Surprisal_TS, ncol = 4) +
                      theme_classic(14) +
                      theme(strip.background.x=element_rect(color = NA))

# save plot
cowplot::save_plot(plot_RT_surprisal_task, file = here::here(paste0("Plots/RT_surprisal_task_", today, ".pdf")))

#---- Plot RTs by Surprisal x Cognitive Load ----
plot_RT_surprisal_cond <- ggplot(plot_surprisal_df, aes(x = Surprisal, y = Reaction_time, fill = Cognitive_Load, color = Cognitive_Load)) +
                      geom_smooth(method = lm, aes(fill = Cognitive_Load), na.rm = T) +
                      #geom_point2(plot_surprisal_df_summary, mapping = aes(x = Cognitive_Load, y = Reaction_time)) +
                      scale_color_manual(values = palet_load) +
                      scale_fill_manual(values = palet_load) +
                      scale_x_continuous(breaks = scales::pretty_breaks(n = 5)) +
                      facet_wrap(~ Surprisal_TS, ncol = 4) +
                      theme_classic(14) +
                      theme(legend.position = "none",
                            strip.background.x=element_rect(color = NA))

# save plot
cowplot::save_plot(plot_RT_surprisal_cond, file = here::here(paste0("Plots/RT_surprisal_condition_", today, ".pdf")))
```


## Plot Accuracy data
d-primes and comprehension questions by Task and Condition
Hint: We didn't have an n-back task in the baseline condition, so we can only compute & plot them for the 1-back and 2-back dual- and single-task blocks.
```{r plot d-primes by condition, age and location, echo = TRUE}
# get data, exclude BL because we didn't have an n-back task there
plot_df <- subset(df_clean, cognitive_load != "Reading only" )
plot_df <- droplevels(plot_df)

# get d-prime data:
# plot_df <- subset(df_text_data, block_kind == "1back_single_main" |
#                                 block_kind == "2back_single_main" |
#                                 block_kind == "1back_main" |
#                                 block_kind == "2back_main")

# aggregate by recording_location, age group, ID and cognitive load condition
plot_df <- setNames(aggregate(plot_df$dprime,
                              by = list(plot_df$participant, plot_df$cognitive_load),
                              FUN = mean),
                    c("participant", "Task", "Mean_dprime"))

# plot d-primes
plot_dprimes <- ggplot(plot_df, aes(x = Task, 
                                    y = Mean_dprime), 
                            width = 4, height = 7) +
                      #geom_point(aes(color = Condition, fill = Condition), 
                      #       position = position_jitter(width = 0.04, height = 0),
                      #       alpha = 0.3,
                      #       shape = 19,
                      #       size = 1) +
                      #geom_violinhalf(flip = c(1,3)) +
                      geom_boxplot() +
                      scale_color_manual(values = palet_dprimes_lines) +
                      scale_fill_manual(values = palet_dprimes) +
                      theme_classic(14) +
                      theme(legend.position=("bottom"))+
                      coord_cartesian(ylim = c(0, 5))
plot_dprimes
cowplot::save_plot(plot_dprimes, file = here::here(paste0("Plots/dprimes_", today, ".pdf")))

## plot comprehension question performance
# Problem here: I excluded all blocks with comprehension questions scores of 0, 
# so if I plot only the cleaned data, the results will look different.
# --> To do: Plot raw data from df_comprehension_Qs here
# which(df_comprehension_Qs$nr_correct == 0)

plot_df <- df_clean

# aggregate by recording_location, age, ID and cognitive load condition
plot_df <- setNames(aggregate(plot_df$question_acc,
                              by = list(plot_df$participant, plot_df$cognitive_load),
                              FUN = mean, na.rm = T),
                    c("participant", "Task", "Mean_percent_correct"))

# calculate percentage
plot_df$Mean_percent_correct <- plot_df$Mean_percent_correct * 100

# plot % correct by condition & age
plot_comprehensionQs <- ggplot(plot_df,
                               aes(x = Task, 
                                   y = Mean_percent_correct)) +
                         geom_violinhalf() +
                         geom_jitter2(mapping = aes(group = participant), width = 0.2) +
                         scale_color_manual(values = palet_task_condition_lines) +
                         scale_fill_manual(values = palet_task_condition) +
                         # scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
                         theme_classic(14) +
                         #coord_cartesian(ylim = c(0, 1),
                        #                xlim = c(18, 85)) +
                         theme(legend.position = "bottom")
plot_comprehensionQs
            
ggsave(plot_comprehensionQs, 
       file = here::here(paste0("Plots/compr_Qs_", today, ".pdf")), 
       device = "pdf", width = 4, height = 4)

```  

#Plots for OA01
```{r}
plot_df <- subset(df_clean, reaction == F)
plot_OA01_df <- subset(plot_df, participant == "OA01")

#---- RTs by Task and Condition ----
plot_RT_task_OA01 <- ggplot(plot_df, aes(x = cognitive_load, y = duration)) +
                      #geom_violinhalf(flip = c(1,3,5)) +
                      geom_boxplot() +
                      geom_boxplot(plot_OA01_df, mapping = aes(x = cognitive_load, y = duration), color = "red", fill = "red", size = 0.1, alpha = 0.4)+
                      #geom_point2(plot_surprisal_df_summary, mapping = aes(x = Cognitive_Load, y = Reaction_time)) +
                      scale_color_manual(values = palet_task_condition_lines) +
                      scale_fill_manual(values = palet_task_condition) +
                      #facet_wrap(~ Surprisal_TS, ncol = 4) +
                      theme_classic(14)
                      #theme(legend.position = "none",
                      #      strip.background.x=element_rect(color = NA))
plot_RT_task_OA01
# save plot
cowplot::save_plot(plot_RT_task_OA01, file = here::here(paste0("Plots/RT_task_OA01_", today, ".pdf")))

#------------------------------------------------------------------------------------------------------------

# get data, exclude BL because we didn't have an n-back task there
plot_df <- subset(df_clean, cognitive_load != "Reading only" )
plot_df <- droplevels(plot_df)
plot_OA01_df <- subset(plot_df, participant == "OA01")

# get d-prime data:
# plot_df <- subset(df_text_data, block_kind == "1back_single_main" |
#                                 block_kind == "2back_single_main" |
#                                 block_kind == "1back_main" |
#                                 block_kind == "2back_main")

# aggregate by recording_location, age group, ID and cognitive load condition
plot_df <- setNames(aggregate(plot_df$dprime,
                              by = list(plot_df$participant, plot_df$cognitive_load),
                              FUN = mean),
                    c("participant", "Task", "Mean_dprime"))
plot_OA01_df <- setNames(aggregate(plot_OA01_df$dprime,
                              by = list(plot_OA01_df$participant, plot_OA01_df$cognitive_load),
                              FUN = mean),
                    c("participant", "Task", "Mean_dprime"))

# plot d-primes
plot_dprimes_OA01 <- ggplot(plot_df, aes(x = Task, 
                                    y = Mean_dprime), 
                            width = 4, height = 7) +
                      #geom_point(aes(color = Condition, fill = Condition), 
                      #       position = position_jitter(width = 0.04, height = 0),
                      #       alpha = 0.3,
                      #       shape = 19,
                      #       size = 1) +
                      #geom_violinhalf(flip = c(1,3)) +
                      geom_boxplot() +
                      geom_boxplot(plot_OA01_df, mapping = aes(x = Task, y = Mean_dprime), color = "red", fill = "red", size = 0.1, alpha = 0.4)+
                      scale_color_manual(values = palet_dprimes_lines) +
                      scale_fill_manual(values = palet_dprimes) +
                      theme_classic(14) +
                      theme(legend.position=("bottom"))+
                      coord_cartesian(ylim = c(0, 5))
plot_dprimes_OA01
cowplot::save_plot(plot_dprimes_OA01, file = here::here(paste0("Plots/dprimes_OA01_", today, ".pdf")))

## plot comprehension question performance
# Problem here: I excluded all blocks with comprehension questions scores of 0, 
# so if I plot only the cleaned data, the results will look different.
# --> To do: Plot raw data from df_comprehension_Qs here
# which(df_comprehension_Qs$nr_correct == 0)

plot_df <- df_clean
plot_OA01_df <- subset(plot_df, participant=="OA01")
# aggregate by recording_location, age, ID and cognitive load condition
plot_df <- setNames(aggregate(plot_df$question_acc,
                              by = list(plot_df$participant, plot_df$cognitive_load),
                              FUN = mean, na.rm = T),
                    c("participant", "Task", "Mean_percent_correct"))
plot_OA01_df <- setNames(aggregate(plot_OA01_df$question_acc,
                              by = list(plot_OA01_df$participant, plot_OA01_df$cognitive_load),
                              FUN = mean, na.rm = T),
                    c("participant", "Task", "Mean_percent_correct"))
# calculate percentage
plot_df$Mean_percent_correct <- plot_df$Mean_percent_correct * 100
plot_OA01_df$Mean_percent_correct <- plot_OA01_df$Mean_percent_correct * 100

# plot % correct by condition & age
plot_comprehensionQs_OA01 <- ggplot(plot_df,
                               aes(x = Task, 
                                   y = Mean_percent_correct)) +
                         geom_violinhalf() +
                         geom_jitter2(mapping = aes(group = participant), width = 0.2) +
                         geom_point2(plot_OA01_df, mapping = aes(x = Task, y = Mean_percent_correct), color="red", fill="red", size = 3, alpha = 0.4)+
                         scale_color_manual(values = palet_task_condition_lines) +
                         scale_fill_manual(values = palet_task_condition) +
                         # scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
                         theme_classic(14) +
                         #coord_cartesian(ylim = c(0, 1),
                        #                xlim = c(18, 85)) +
                         theme(legend.position = "bottom")
plot_comprehensionQs_OA01
            
ggsave(plot_comprehensionQs_OA01, 
       file = here::here(paste0("Plots/compr_Qs_OA01_", today, ".pdf")), 
       device = "pdf", width = 4, height = 4)

``` 
```

```{r}
lmm_df <- subset(df_clean, !is.na(surprisal_60))
m1 <- lmer(log(duration) ~ surprisal_1 * cognitive_load + (cognitive_load|participant), data = lmm_df)

plot(ggemmeans(m1, terms = c("surprisal_1", "cognitive_load")))
```
