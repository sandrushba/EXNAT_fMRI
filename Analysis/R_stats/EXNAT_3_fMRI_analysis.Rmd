---
title: "EXNAT_3_fMRI_analysis"
author: "Sandra Martin"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r file setup, echo = FALSE}
rm(list = ls()) # clear environment
knitr::opts_chunk$set() # set default options for all code blocks in this document
options(scipen = 999) # don't use scientific notation for very large or small numbers
```

# Load packages
```{r packages, echo = FALSE, message = FALSE, warning = TRUE}
# Create a list with needed libraries
pkgs <- c("here", # for working with relative paths
          "ggeffects", # create data frames of marginal effects for 'ggplot' from model outputs
          "stringr", # for getting substrings
          "ggplot2", # for plots
          "dplyr", # for replacing multiple values in vector with different values
          "reshape2", # for reshaping df format
          "tidyverse", # for aggregating
          "psycho", # for computing d-prime values
          "devtools", # for getting packages from github
          "gtools", # for getting tuples from list
          "see", # for plotting residuals of lme4 model
          "performance", # needed for plotting the lme4 model output summary
          "lme4", # also for linear mixed models, but no p-values in the summary
          "lmerTest", # extension of lme4 - for tab_model() for showing lme4 model results
          "modelsummary", # for printing model fit measures
          "car", # for basic Anova() function & aGSIFs (= basically weird adjusted VIFs)
          "sjPlot", # for running Anova & showing results in a HTML table in the Viewer
          "colorspace", # for beautiful and balanced color palettes
          "interactions") # for simple slopes analysis


# Load each listed library, check if it's already installed
# and install if necessary
for (pkg in pkgs){
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

# install package with lists of stop words from github
devtools::install_github("quanteda/stopwords")
library(stopwords)
```

# Load d-prime function from different R script:
```{r source d-prime function, echo = FALSE}
source(here::here("get_dprime.R")) # get script
```


# Settings for plots
```{r Theme for plots, echo = FALSE}
apatheme <- theme_bw()+
  theme(plot.title = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        text = element_text(family = 'sans',size = 14)) #panel.grid.major=element_blank(),

today <- Sys.Date()
today <- format(today, format="%y%m%d")


# Custom colour palettes for the plots:

# for distinguishing between paced and unpaced task:
palet_task_condition <- c("#D2BFE7",
                      "#977FB2") 
palet_task_condition_lines <- c("#ad8bd3",
                      "#7B5EC6") 

# for distinguishing between cognitive load conditions: 
# palet_load <- c("#575A7B", # Reading Only
#                 "#ED9201", # 1-back
#                 "#982126") # 2-back
palet_load <- c("#4B201D", "#9F4D48", "#F8A29E")

# for distinguishing between n-back tasks (single- & dual 1- and 2-back)
palet_dprimes = c("#ED9201", # 1-back Dual
                  "#fec160", # 1-back Single
                  "#982126", # 2-back Dual
                  "#d95459") # 2-back Single
palet_dprimes_lines = c("#cb7d00", # 1-back Dual
                        "#ED9201", # 1-back Single
                        "#982126", # 2-back Dual
                        "#d95459") # 2-back Single

# for simple slopes plots:
palet_simple_slopes = c("#585858", # n.s.
                        "#0c6ae8") # sign.

# For effects we don't really have colours for. Like age or surprisal.
palet_effects <- c("#365464") 

```


# Set path to datasets
```{r path setup, echo = FALSE}
# online and lab data should be all in 1 folder:
path_data_folder <- here::here("Raw_data/fMRI_experiment/")
```

# Read in data & do preprocessing on participant-level
In this chunk, we read in every dataset individually, compute reading times, reading speed, d-primes, comprehension question performance, and so on, 
add a few information on the words we used (e.g. surprisal scores on our 4 time scales, word frequencies, word lengths, punctuation,...), 
and then we exclude data on block level:
We only exclude data on block-level if performance measures indicate they didn't do the task(s) correctly.
```{r Read in data, message=FALSE, echo=FALSE}
# Get list of all .csv files in the data folder
file_list <- list.files(path = path_data_folder, pattern='.csv')

# Placeholders df for demographics, questions & text data
df_all_data        <- data.frame()
df_comprehension_Qs <- data.frame()

# Loop files in my file list aka directory
for (i in 1:length(file_list)) {

  # PREPARE FILE FOR PREPROCESSING
  # Read in current file
  subj_df <- read.csv(paste(path_data_folder, file_list[i],sep = ""), sep = ",")

  # If the df is separated by a semicolon instead of a comma, we only get
  # one messed-up column. In this case, use semicolon as separator for csv:
  if (ncol(subj_df) == 1) {
   subj_df <- read.csv(paste(path_data_folder, file_list[i],sep = ""), sep = ";")
  }
  
  id <- subj_df$participant[1]
  
  # print message
  message(paste(i, " - Reading in file ", file_list[i], ", participant ID: ", id, sep = ""))

  # add column with info on whether participant should be excluded
  excl <- FALSE
  
  # rename block_kind column
  subj_df <- subj_df %>% 
    rename(nback_level = n.back_level)
  

  # GET RAW TEXT & N-BACK DATA

  # get words & information on the text from data
  subj_df <- subj_df[, c("colour", "target", "nback_response", "nback_RT", "duration", "text_nr", "trial_nr", "block_nr_exp", "run_nr", "block_nr_run", "block_name", "nback_level", "word", "question", "button_pressed", "chosen_ans", "ans_correct", "RT_per_rectangle_oneback_single", "RT_per_rectangle_twoback_single", "RT_per_letter_baseline", "RT_per_letter_oneback_dual", "RT_per_letter_twoback_dual", "participant", "session", "date")]
  
  # remove weird empty rows
  subj_df <- subset(subj_df, !is.na(run_nr))

  ### ADD NUMBERED BLOCK NAMES ####

  # # We have each main block twice, but I would like to exclude outliers by block and not by condition.
  # # Reason: Reading times in second block might be slightly different than in first block, so don't mix them up.
  # 
  # change_df <- subj_df %>% 
  #   filter(!is.na(trial_nr)) %>% 
  #   group_by(block_name, block_nr) %>% 
  #   count() %>% 
  #   ungroup() %>% 
  #   group_by(block_name) %>% 
  #   mutate(Index=1:n()) %>% 
  #   mutate(block_names_numbered = paste(block_name, Index, sep = "_")) %>% 
  #   select(-c(Index, n))
  # 
  # subj_df <- full_join(subj_df, change_df)


  ### MERGE INFO ON RT PER LETTER INTO ONE COLUMN ####
  subset_df <- subj_df %>% 
    select(c(block_name, starts_with("RT_per_"))) %>% 
    mutate(Paced_RT = case_when(
      block_name == "Reading_Baseline_main_no_click" ~ RT_per_letter_baseline[!is.na(subj_df$RT_per_letter_baseline)][1],
      block_name == "Reading_pseudotext_no_click" ~ RT_per_letter_baseline[!is.na(subj_df$RT_per_letter_baseline)][1],
      block_name == "1back_dual_main_no_click" ~ RT_per_letter_oneback_dual[!is.na(subj_df$RT_per_letter_oneback_dual)][1],
      block_name == "2back_dual_main_no_click" ~ RT_per_letter_twoback_dual[!is.na(subj_df$RT_per_letter_twoback_dual)][1],
      block_name == "1back_single_main_no_click" ~ RT_per_rectangle_oneback_single[!is.na(subj_df$RT_per_rectangle_oneback_single)][1],
      block_name == "2back_single_main_no_click" ~ RT_per_letter_twoback_dual[!is.na(subj_df$RT_per_letter_twoback_dual)][1],
    )) %>% 
    select(c(block_name, Paced_RT)) %>% 
    distinct()
  
  subj_df <- full_join(subj_df, subset_df)
  
  
  # GET ADDITIONAL INFORMATION ON THE TEXTS:

  ### PUNCTUATION ####

  # Edit the texts a bit. Currently, we have words & punctuation
  # mixed up. Would be nice if we had one word column
  # and some others with info on punctuation.

  punctuation <- c(rep("", times = length(subj_df$participant)))

  # get all .
  punctuation[grep("[.]", subj_df$word)] <- "point"
  # get all ?
  punctuation[grep("[?]", subj_df$word)] <- "question_mark"
  # get all !
  punctuation[grep("[!]", subj_df$word)] <- "exclamation_mark"
  # get all ,
  punctuation[grep("[,]", subj_df$word)] <- "comma"
  # get all ;
  punctuation[grep("[;]", subj_df$word)] <- "semicolon"
  # get all :
  punctuation[grep("[:]", subj_df$word)] <- "colon"

  # get all "
  # This is tricky for various reasons:

  # 1. there could be quotes directly after or
  # before a point for example

  # Idea: Separate column for quotes
  quotes <- c(rep("", times = length(subj_df$participant)))

  # 2. I want to differentiate between opening and closing quotes,
  # but they look the same.

  # Idea: make them all "opening quotes" and change every second one to
  # "closing_quote". This should do the trick.
  quotes[grep("\"", subj_df$word)] <- "opening_quote"
  opening <- TRUE

  # loop list of quotes
  for (x in 1:length(quotes)){
    # get current value in vector "quotes" & current word
    curr_val  <- quotes[x]
    curr_word <- subj_df$word[x]

    if (curr_val == "opening_quote" & # if the current value is not empty
        opening == TRUE & # and we set "opening" to TRUE
        grepl("\"[A-Za-z0-9]+\"", curr_word) == FALSE){ # and there are not 2 quotes around the word

      # leave it as is, next quote is the closing one
      opening <- FALSE

      # if we have a quote, but it has to be a closing one, change it accordingly
    } else if (curr_val == "opening_quote" &
               opening == FALSE &
               grepl("\"[A-Za-z0-9]+\"", curr_word) == FALSE){
      # change value to closing quote
      quotes[x] <- "closing_quote"
      # next quote is the opening one
      opening <- TRUE
      # if it's a quote around a single word, mark as "both"
    } else if (curr_val == "opening_quote" &
               grepl("\"[A-Za-z0-9]+\"", curr_word) == TRUE){
      # change value to both
      quotes[x] <- "both"
    }
  }

  # remove all punctuation
  word_single <- gsub('[[:punct:] ]+',' ', subj_df$word)
  # "trim" away spaces before and after words
  word_single <- trimws(word_single, which = c("left"))
  word_single <- trimws(word_single, which = c("right"))
  # put dashes into spaces between words (words like "ice-cream" for example)
  word_single <- gsub(" ", "-", word_single, fixed = TRUE)

  # get word length for single words
  word_length_single <- nchar(word_single)


  ### N-BACK RESPONSES ####

  # check if we had an n-back reaction while the word was shown
  # create vector with only FALSEs
  reaction <- c(rep(FALSE, times = length(subj_df$participant)))
  # check where in subj_df we have responses (hit or false alarm) and change value in reaction vector to TRUE at that index
  reaction[which(subj_df$nback_response == "hit" | subj_df$nback_response == "false alarm")] <- TRUE

  # APPEND NEW COLUMNS
  # append punctuation and quotes column, column with words without punctuation
  # column with length of each the single words and reaction column as
  # additional columns to subj_df:
  subj_df <- data.frame(cbind(subj_df, word_single, punctuation,
                              quotes, word_length_single, reaction))

  ### STOP WORDS ####

  # Now also get previous reaction and stop words.
  # First, append empty columns to df:
  subj_df[, c("previous_reaction", "previous_duration", "stop_word")] <- ""

  # get list of German stop_words
  stop_words <- stopwords("de", source = "snowball")
  # snowball = stopwords list based on the Snowball stemmer's word lists.
  # --> https://snowballstem.org/texts/introduction.html

  # now loop rows and gradually fill in the empty columns.
  for (idx in 1:length(subj_df$participant)){
    # if it's the first trial of a new block, go to next iteration
    # because in this case we don't have previous data.
    # Also skip the following part if it's a block without text.
    if (subj_df$trial_nr[idx] > 1 && !is.na(subj_df$trial_nr[idx])){
      # fill in the missing data by getting the values from the previous trial:
      subj_df$previous_duration[idx]    <- subj_df$duration[idx-1]
      subj_df$previous_reaction[idx]    <- subj_df$reaction[idx-1]
    }

    if (subj_df$word_single[idx] %in% stop_words){
      # check if current word is a stop word or not
      subj_df$stop_word[idx] <- TRUE 
      }
    else {
      subj_df$stop_word[idx] <- FALSE
    }
  }
  
  ### WORD FREQUENCIES & SURPRISAL SCORES ####

  # Include word frequencies & surprisal scores for each word

  # append "empty" word frequency & surprisal score columns to df
  # to do so, create a vector of column names
  col_names <- c("word_frequency", paste0("surprisal_", c(1, 4, 12, 60)))#,
                 #paste0("surprisal_", c(1, 2, 8, 16, 32), "_ortho"),
                 #paste0("similarity_", c(1, 2, 8, 16, 32)))
  # then add "empty" columns to data frame subj_df
  subj_df[, col_names] <- 0

  # get word frequency & surprisal scores for each word
  # load word freq df
  word_freqs_df = read.csv(here::here("word frequencies/Word_freqs.csv"), sep = ";", header = TRUE)[2:4]
  # Explanation: We compute the word frequencies using another
  #              script which is called "calculate_word_frequencies.py".
  #              You can find it in the Analysis folder.
  #              --> word frequencies were taken from this python package:
  #              Speer, R., Chin, J., Lin, A., Jewett, S., & Nathan, L. (2018, October 3).
  #              LuminosoInsight/wordfreq: v2.2. Zenodo. https://doi.org/10.5281/zenodo.1443582


  # Load surprisal scores
  # load surprisal scores / similarity scores df with TS = context chunk size
  surprisal_df = read.csv(here::here("surprisal scores/surprisal_scores_masked_context.csv"), sep = ",", header = TRUE)

  # Explanation: I computed the scores in Python using a German GPT-2 model. For each text you can select a context chunk of x words (e.g. 5 words)
  #              and predict the next word. For each possible continuation of your context, you get probabilities.
  #              If you get the probability for the actual word and compute the negative log of it, you have the surprisal score
  #              for your word on time scale x (e.g. 5 words = TS 5). The time scales are all highly correlated, which might be due to the fact that each
  #              time scale also includes context information from all lower timescales, so it's like a Russian doll situation.
  #              To deal with this problem, we masked all words that were already processed in lower time scales,
  #              so each time scale only uses the "new" parts of the input chunk.


  # loop individual texts
  for (curr_text_nr in unique(subj_df$text_nr)){

    # in some blocks we don't have texts, so skip those
    # CAVE: currently also skipping text 10 because I still need to calculate values for this text
    if (curr_text_nr == "" | curr_text_nr == "text_10" | startsWith(curr_text_nr, "pseudo")){
      next
      # if it's a text block, though, assign word frequencies from csv
    } else {
      #print(curr_text_nr) # uncomment this if you'd like to show the texts each participant read

      # get word frequencies for current text nr
      curr_word_freqs <- subset(word_freqs_df, text_nr == curr_text_nr)$word_frequency
      # find out where in the subj_df text the text is located and add the word frequencies there
      subj_df[which(subj_df$text_nr == curr_text_nr & subj_df$question == ""),]$word_frequency <- curr_word_freqs

      # Do the same for the surprisal scores.
      curr_surprisals <- subset(surprisal_df, text_nr == curr_text_nr)

      # find out where in the subj_df text the current text nr is located
      curr_row <- which(subj_df$text_nr == curr_text_nr & subj_df$question == "")

      # add the surprisal scores (untransformed & orthogonalized scores) there
      subj_df[curr_row, c("surprisal_1", "surprisal_4",
                          "surprisal_12", "surprisal_60")]  <- curr_surprisals[c("surprisal_1", "surprisal_4",
                                                                                 "surprisal_12",  "surprisal_60")]
      #subj_df[curr_row, c("surprisal_1_ortho", "surprisal_2_ortho",
      #                    "surprisal_8_ortho", "surprisal_16_ortho",
      #                    "surprisal_32_ortho")]                     <- curr_surprisals[c("surprisal_1_ortho", "surprisal_2_ortho",
      #                                                                                    "surprisal_8_ortho", "surprisal_16_ortho",
      #                                                                                    "surprisal_32_ortho")]

    }# END if
  }# END loop texts

  
  ### GET SURPRISAL SCORES OF PREVIOUS WORD ####
  # --> basically do the same again as before, but add surprisal scores for current for to row of next word.

  # append "empty" word frequency & surprisal score columns to df
  # to do so, create a vector of column names
  col_names <- c("word_frequency_previous_word", paste0("previous_surprisal_", c(1, 4, 12, 60)))
  # then add "empty" columns to data frame subj_df
  subj_df[, col_names] <- 0

  # loop individual texts
  for (curr_text_nr in unique(subj_df$text_nr)){

    # in some blocks we don't have texts, so skip those
    if (curr_text_nr == "" | curr_text_nr == "text_10" | startsWith(curr_text_nr, "pseudo")){
      next
      # if it's a text block, though, assign word frequencies from csv
    } else {
      #print(curr_text_nr) # uncomment this if you'd like to show the texts each participant read

      # get word frequencies for current text nr
      curr_word_freqs <- subset(word_freqs_df, text_nr == curr_text_nr)$word_frequency

      # now this is where we do it differently than before: Remove the last value and add 1 NA
      # at idx = 1 of the vector, so the values are moved by 1 position.
      curr_word_freqs <- c(NA, curr_word_freqs[-length(curr_word_freqs)])

      # find out where in the subj_df text the text is located and add the word frequencies there
      subj_df[which(subj_df$text_nr == curr_text_nr & subj_df$question == ""),]$word_frequency_previous_word <- curr_word_freqs

      # Do the same for the surprisal scores:
      curr_surprisals <- subset(surprisal_df, text_nr == curr_text_nr)[ , c("surprisal_1", "surprisal_4",
                                                                            "surprisal_12", "surprisal_60")]

      # add 1 NA to the start of the vectors, remove last value in each
      curr_surprisals <- rbind(c(NA, NA, NA, NA), curr_surprisals[-nrow(curr_surprisals), ])

      # find out where in the subj_df text the current text nr is located
      curr_row <- which(subj_df$text_nr == curr_text_nr & subj_df$question == "")

      # add the surprisal scores (untransformed & orthogonalized scores) there
      subj_df[curr_row, c("previous_surprisal_1", "previous_surprisal_4",
                          "previous_surprisal_12","previous_surprisal_60")] <- curr_surprisals

    }# END if
  }# END loop texts

  ### CHANGE ORDER OF DATAFRAME ####
  # Move around the columns a bit:
  col_order <- c("participant", "nback_level", "block_nr_exp", "run_nr", "block_nr_run", "block_name",
                 "trial_nr", "text_nr", "word", "duration",
                 "colour", "target", "button_pressed", "nback_response", "nback_RT", "reaction",
                 "word_single", "word_length_single", "word_frequency", "Paced_RT",
                 "surprisal_1", "surprisal_4", 
                 "surprisal_12", "surprisal_60",
                 "previous_surprisal_1", "previous_surprisal_4",
                 "previous_surprisal_12","previous_surprisal_60",
                 "word_frequency_previous_word", 
                 "previous_duration", "previous_reaction",
                 "session", "question", "chosen_ans", "ans_correct", "stop_word", "punctuation", "quotes", "date")

  subj_df <- subj_df[, col_order]
  
  ### GET PERFORMANCE MEASURES ####

  #### COMPREHENSION QUESTION PERFORMANCE ####
  # Check the performance in the reading comprehension questions in the 2 baseline blocks:
  # If they don't have 3/3 in at least one of the blocks, exclude their data.

  # get question data:
  Q_df <- subset(subj_df, question != "")[,c("question", "chosen_ans", "ans_correct", "text_nr", "nback_level", "participant")]

  Q_df$ans_correct <- as.logical(Q_df$ans_correct)
  
  # calculate accuracy per text
  Q_df <- Q_df %>% 
    group_by(text_nr) %>% 
    mutate(question_acc = sum(ans_correct[ans_correct == TRUE])/3)
  
  Q_subset <- Q_df %>% 
    select(c(text_nr, question_acc)) %>% 
    distinct

  # append Q_subset to bigger df for all participants
  df_comprehension_Qs <- as.data.frame(rbind(df_comprehension_Qs, Q_df))
  
  # append overall accuracy per text to subject df
  subj_df <- full_join(subj_df, Q_subset) %>% 
    select(-c(question, chosen_ans, ans_correct)) %>% 
    filter(!is.na(duration))


  ### N-BACK PERFORMANCE ####

  # set all durations in n-back trials without response to NA
  subj_df[which(subj_df$reaction == F), ]$nback_RT <- NA

  # create vector of n-back blocks in experiment
  nback_block_names <- subj_df %>% 
    filter(str_detect(nback_level, "^1") | str_detect(nback_level, "^2")) %>% 
    distinct(block_nr_exp) %>% 
    pull(block_nr_exp)

  # append empty d-prime column to df:
  subj_df$dprime <- c(rep(NA, times = length(subj_df$word)))

  # loop block names:
  for (nback_block in nback_block_names){

    # get data for current block
    curr_block <- subset(subj_df, block_nr_exp == nback_block)

    # remove trials where the RT was way too fast
    # (= participant reacted by accident)
    curr_block <- subset(curr_block, (nback_RT >= 100 | is.na(nback_RT)))

    # if there are still some trials left (let's say at least 10), compute d-prime
    if (length(curr_block$participant) > 10){
        # compute d-prime & add to bigger df
        d_prime <- get_dprime(curr_block$nback_response)
        subj_df$dprime[subj_df$block_nr_exp == nback_block] <- d_prime
    } # END if loop - check if there are still enough trials left
  }# END loop - compute d-primes
  
  # We'll use the mean d-prime from 1-back and 2-back single task main blocks
  # as a working memory measure for each participant. So get mean & append it as a new column to the df.
  subj_df$mean_dprime_singletasks <- mean(c(unique(subj_df[which(subj_df$block_name =="1back_single_main_no_click"),]$dprime),
                                            unique(subj_df[which(subj_df$nback_level =="2back_single_main_no_click"),]$dprime)))
  
  ### MARK PARTICIPANTS/BLOCKS FOR EXCLUSION:

  # append "empty" exclude trial / exclude participant columns to df
  subj_df$excl_trial       <- FALSE

  ### EXCLUDE SINGLE BLOCKS BASED ON COMPREHENSION QUESTION PERFORMANCE: ####
  
  # Idea: only exclude on block level, don't exclude full datasets because they messed up in 1 condition or so.
  # To identify these blocks, we look for a question accuracy of 0

  subj_df$excl_trial[subj_df$question_acc == 0] <- TRUE
  message(paste("excluded", length(subj_df[which(subj_df$question_acc == 0), "excl_trial"]), "trials because accuracy was 0%", sep = " "))
  
  ### EXCLUDE SINGLE BLOCKS BASED ON N-BACK TASK PERFORMANCE: ####

  # Check blocks with n-back tasks: 
  # Exclude block if the participant didn't do the n-back task (i.e. if they always/never pressed the target button).
  # We're excluding based on the d-prime values.
  # In the dual-task (main) blocks, we always had 50 targets & 250 non-targets. 
  # The d-prime for 0 hits and 0 false alarms is 0. If they always pressed the button and have 50 hits and 250 false alarms, the d-prime is also 0.
  # This means that they should have a d-prime above 0 if they at least attempted to do the task or below 0 if they attempted to do the 
  # task but did it the wrong way around (i.e.reacted if trial was a non-target trial and didn't react if trial was a target trial).
  
  # --> exclude all blocks with d-primes == 0.
  
  message(paste("excluded", length(subj_df[which(subj_df$dprime == 0), "excl_trial"]), "trials because d-prime was 0", sep = " "))
  subj_df[which(subj_df$dprime == 0), "excl_trial"] <- TRUE

  # append subj_df chunk to df_text_data where we collect the data of all participants we want to keep
  df_all_data <- as.data.frame(rbind(df_all_data, subj_df))

  message("------------------------")

}# END READ IN DATASETS

# exclude all trials that we marked for exclusion:
df_all_data <- subset(df_all_data, excl_trial == FALSE)

# clean up!
rm(list=ls()[! ls() %in% c("df_all_data", "df_comprehension_Qs", 
                           "apatheme", "palet_load", 
                           "palet_dprimes", "palet_dprimes_lines", 
                           "palet_lab_online", "palet9", "today")])
```

# Save raw data in folder "RData"
```{r save raw data, echo = FALSE}
# save df
save(df_all_data, file = here::here("RData/df_all_data.RData"))

# save comp question df
save(df_comprehension_Qs, file = here::here("RData/df_comprehension_questions.RData"))
```
